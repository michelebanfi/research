| ID | Category | Requirement | Description | Acceptance Criteria |
| --- | --- | --- | --- | --- |
| **REQ-DATA-01** | **Data Integrity** | **Strict Graph Entity Resolution** | Update the graph extraction prompt in `src/ai_engine.py` to return `source_type` and `target_type` for edges. Update `store_graph_data` in `schema.sql` to resolve node IDs using both `name` AND `type`. | Graph edges correctly distinguish between entities with the same name but different types (e.g., "Apple" the Company vs. "Apple" the Fruit). |
| **REQ-SEC-01** | **Security** | **Sandbox Static Analysis** | Implement a pre-execution AST scan in `src/sandbox.py`. The scanner must block code containing dangerous imports (`os`, `sys`, `subprocess`, `shutil`) before passing it to `asyncio.subprocess`. | Submitting code like `import os; os.system('ls')` returns a "Security Violation" error immediately and does not execute. |
| **REQ-PERF-01** | **Performance** | **Bulk Graph Operations** | Refactor `store_graph_data` in `schema.sql` to remove the iterative `FOR` loop. Replace with set-based SQL operations (using `UNNEST` and Common Table Expressions) to insert all nodes and edges in a single transaction. | Ingestion time for large documents decreases. Database logs show single `INSERT` statements rather than hundreds of sequential inserts. |
| **REQ-POETIQ-06** | **Reasoning** | **Executable Verification** | Upgrade `ReasoningAgent` in `src/reasoning_agent.py` to generate a Python assertion script instead of performing string-based verification. The agent must execute this script in the sandbox to confirm the task result. | The agent performs two sandbox executions per attempt (generation + verification). Verification fails if the assertion script raises an `AssertionError`. |
| **REQ-STABILITY-01** | **Stability** | **Context Window Management** | Add token estimation logic to `chat_with_context_async` in `src/ai_engine.py`. Automatically truncate chat history or retrieved context chunks if the total exceeds the model's limit (e.g., 4096 tokens). | Conversations with long histories or large `top_k` retrievals do not result in LLM API errors regarding context length. |