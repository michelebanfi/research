import streamlit as st
import asyncio
import os
from pathlib import Path
from src.agent import ResearchAgent
from src.models import ReasoningResponse

def render_chat_tab(db, ai):
    """
    Renders the Chat tab content.
    """
    st.subheader("Chat with your Knowledge Base")
    
    # Two-column layout: Chat (70%) | Context Panel (30%)
    chat_col, context_col = st.columns([7, 3])
    
    with chat_col:
        # Chat settings
        with st.expander("‚öôÔ∏è Chat Settings", expanded=False):
            do_rerank = st.checkbox("Enable Re-ranking", value=True, help="Uses FlashRank to improve context relevance")
            top_k = st.slider("Number of context chunks", min_value=3, max_value=10, value=5)
            reasoning_mode = st.toggle(
                "üß† Reasoning Mode", 
                value=False, 
                help="Enable Plan & Code mode: the model will plan, write code, execute it in a sandbox, and iterate on errors"
            )
        
        # Display chat history
        chat_container = st.container()
        with chat_container:
            for msg in st.session_state.chat_history:
                with st.chat_message(msg["role"]):
                    st.markdown(msg["content"])
                    if msg.get("model"):
                        st.caption(f"Generated by: `{msg['model']}`")
        
        # Chat input
        if prompt := st.chat_input("Ask a question about your knowledge base..."):
            # Add user message to history
            st.session_state.chat_history.append({"role": "user", "content": prompt})
            
            with st.chat_message("user"):
                st.markdown(prompt)
            
            with st.chat_message("assistant"):
                # REQ-UI-02: Status container for tool execution visibility
                status_container = st.status("ü§î Thinking...", expanded=True)
                
                # Track tool usage for status updates
                tools_used = []
                
                def update_status(tool_name: str, phase: str):
                    """Callback for tool execution visibility."""
                    if phase == "start":
                        status_container.update(label=f"üîß Using {tool_name}...")
                        status_container.write(f"üìç Calling `{tool_name}`...")
                    elif phase == "complete":
                        status_container.write(f"‚úÖ `{tool_name}` complete")
                        tools_used.append(tool_name)
                
                try:
                    agent = ResearchAgent(
                        ai_engine=ai,
                        database=db,
                        project_id=st.session_state.selected_project_id,
                        status_callback=update_status,
                        do_rerank=do_rerank
                    )
                    
                    # Run the agent
                    result = asyncio.run(agent.run(
                        prompt,
                        st.session_state.chat_history[:-1],  # Exclude current message
                        reasoning_mode=reasoning_mode
                    ))
                    
                    # Update status to complete
                    if tools_used:
                        status_container.update(label=f"‚ú® Done (used: {', '.join(tools_used)})", state="complete", expanded=False)
                    else:
                        status_container.update(label="‚ú® Done", state="complete", expanded=False)
                    
                    # Check if it's a ReasoningResponse or AgentResponse
                    if isinstance(result, ReasoningResponse):
                        # Reasoning mode response
                        if result.plan:
                            with st.expander("üìã Plan", expanded=False):
                                st.markdown(f"**Goal:** {result.plan.goal}")
                                st.markdown(f"**Context needed:** {result.plan.context_needed}")
                                st.markdown(f"**Verification:** {result.plan.verification_logic}")
                        
                        # Show attempts
                        if result.attempts:
                            with st.expander(f"üîÑ Code Attempts ({len(result.attempts)})", expanded=False):
                                for attempt in result.attempts:
                                    status_icon = "‚úÖ" if attempt.success else "‚ùå"
                                    st.markdown(f"**Attempt {attempt.attempt_number}** {status_icon}")
                                    st.code(attempt.code, language="python")
                                    if attempt.output:
                                        st.text(f"Output: {attempt.output[:500]}")
                        
                        # Final answer
                        if result.success:
                            assistant_message = f"**Result:**\n```\n{result.final_output}\n```"
                        else:
                            assistant_message = f"‚ùå {result.error}\n\nLast output:\n```\n{result.final_output or 'No output'}\n```"
                    else:
                        # Standard AgentResponse
                        # Store context for display panel
                        st.session_state.last_context = result.retrieved_chunks
                        st.session_state.matched_concepts = result.matched_concepts
                        assistant_message = result.answer
                    
                except Exception as e:
                    status_container.update(label="‚ùå Error", state="error")
                    assistant_message = f"I encountered an error: {str(e)}"
                    import traceback
                    print(f"Agent error: {traceback.format_exc()}")
                
                st.markdown(assistant_message)
                
                # Show model attribution
                model_used = getattr(result, "model_name", "unknown")
                st.caption(f"Generated by: `{model_used}`")
                
                st.session_state.chat_history.append({
                    "role": "assistant", 
                    "content": assistant_message,
                    "model": model_used  # Store in history for persistence if needed later
                })
        
        # Clear chat button
        if st.session_state.chat_history:
            if st.button("üóëÔ∏è Clear Chat", key="clear_chat"):
                st.session_state.chat_history = []
                st.session_state.last_context = []
                st.rerun()
    
    with context_col:
        render_context_panel()

def render_context_panel():
    """Renders the right-hand context panel."""
    st.markdown("### üìö Retrieved Context")
    
    # REQ-01: Show matched graph concepts
    if st.session_state.get('matched_concepts'):
        with st.expander("üï∏Ô∏è Graph-Matched Concepts", expanded=True):
            concepts = st.session_state.matched_concepts[:8]  # Limit display
            st.markdown(" ‚Ä¢ ".join([f"`{c}`" for c in concepts]))
    
    if st.session_state.last_context:
        for i, chunk in enumerate(st.session_state.last_context):
            # Determine source type
            source_type = chunk.get('source', 'vector')
            source_icon = "üîç" if source_type == 'vector' else "üï∏Ô∏è"
            
            with st.expander(f"{source_icon} Source {i+1}", expanded=(i == 0)):
                # Show scores and source
                original_sim = chunk.get('similarity', 0)
                rerank_score = chunk.get('rerank_score')
                
                # Source badge
                source_badge = "Vector Search" if source_type == 'vector' else "Graph Retrieval"
                st.caption(f"**{source_badge}**")
                
                if rerank_score is not None:
                    st.caption(f"Original: `{original_sim:.3f}` ‚Üí Re-ranked: `{rerank_score:.3f}`")
                else:
                    st.caption(f"Similarity: `{original_sim:.3f}`")
                
                st.markdown(chunk.get('content', '')[:500] + ("..." if len(chunk.get('content', '')) > 500 else ""))
                
                # REQ-VIS-04 / REQ-VIS-05: View Source Buttons
                file_path = chunk.get('file_path')
                metadata = chunk.get('metadata', {})
                
                if file_path:
                    # Construct local static link
                    if "static/" in file_path:
                        relative_path = file_path.split("static/")[1]
                    else:
                        # Fallback: Check if file exists in static/uploads/
                        possible_upload_path = Path("static/uploads") / file_path
                        if possible_upload_path.exists():
                            relative_path = f"uploads/{file_path}"
                        else:
                            relative_path = os.path.basename(file_path) 
                        
                    file_url = f"app/static/{relative_path}"
                    
                    # Layout buttons
                    col1, col2 = st.columns([1, 1])
                    
                    # 1. View PDF
                    if file_path.lower().endswith('.pdf'):
                        page = metadata.get('page_number', 1)
                        with col1:
                            st.link_button("üìÑ View PDF", f"{file_url}#page={page}")
                            
                    # 2. View Code / Text
                    elif file_path.lower().endswith(('.py', '.md', '.txt')):
                        start_line = metadata.get('start_line')
                        end_line = metadata.get('end_line')
                        
                        with col2:
                            if st.button("üìù View Code", key=f"view_{i}_{chunk.get('id')}"):
                                show_code_dialog(file_path, start_line, end_line)

def show_code_dialog(file_path, start_line, end_line):
    with st.dialog("Source Code", width="large"):
        try:
            with open(file_path, 'r') as f:
                lines = f.readlines()
                
            # Highlight specific lines
            if start_line and end_line:
                # Provide some context around lines
                ctx_start = max(0, start_line - 5)
                ctx_end = min(len(lines), end_line + 5)
                
                code_segment = "".join(lines[ctx_start:ctx_end])
                st.caption(f"Showing lines {ctx_start+1} - {ctx_end}")
                st.code(code_segment, language='python' if file_path.endswith('.py') else 'markdown')
            else:
                st.code("".join(lines), language='python' if file_path.endswith('.py') else 'markdown')
                
        except Exception as e:
            st.error(f"Could not read file: {e}")
