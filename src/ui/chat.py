import streamlit as st
import asyncio
import os
from pathlib import Path
from src.agent import ResearchAgent
from src.models import ReasoningResponse

def render_chat_tab(db, ai):
    """
    Renders the Chat tab content.
    """
    st.subheader("Chat with your Knowledge Base")
    
    # Two-column layout: Chat (50%) | Process/Context (50%)
    col1, col2 = st.columns([1, 1])
    
    # Initialize Right Column first to get container reference
    with col2:
        tab_process, tab_context = st.tabs(["üß† Live Process", "üìö Context"])
        
        with tab_process:
            from src.ui.process import render_process_monitor
            # Container for live updates
            process_container = st.empty()
            
            # Initial render
            with process_container.container():
                render_process_monitor()

        with tab_context:
            render_context_panel()

    with col1:
        st.subheader("üí¨ Chat")
        # Chat settings
        with st.expander("‚öôÔ∏è Chat Settings", expanded=False):
            do_rerank = st.checkbox("Enable Re-ranking", value=True, help="Uses FlashRank to improve context relevance")
            top_k = st.slider("Number of context chunks", min_value=3, max_value=10, value=5)
            reasoning_mode = st.toggle(
                "üß† Reasoning Mode", 
                value=False, 
                help="Enable Plan & Code mode: the model will plan, write code, execute it in a sandbox, and iterate on errors"
            )
        
        # Display chat history
        chat_container = st.container(height=600)
        with chat_container:
            for msg in st.session_state.chat_history:
                with st.chat_message(msg["role"]):
                    st.markdown(msg["content"])
                    if msg.get("model"):
                        st.caption(f"Generated by: `{msg['model']}`")
        
        # Chat input
        if prompt := st.chat_input("Ask a question about your knowledge base..."):
            # Add user message to history
            st.session_state.chat_history.append({"role": "user", "content": prompt})
            
            with chat_container:
                with st.chat_message("user"):
                    st.markdown(prompt)
                
                with st.chat_message("assistant"):
                    message_placeholder = st.empty()
                    status_placeholder = st.empty()
                    
                    # Clear previous events for new run
                    st.session_state.agent_events = []
                    
                    # Callback for Live Updates
                    def on_event(event: dict):
                        st.session_state.agent_events.append(event)
                        # Live update the right column
                        with process_container.container():
                            render_process_monitor()
                            
                    try:
                        status_placeholder.markdown("ü§î **Thinking...**")
                        
                        agent = ResearchAgent(
                            ai_engine=ai,
                            database=db,
                            project_id=st.session_state.selected_project_id,
                            do_rerank=do_rerank,
                            event_callback=on_event
                        )
                        # Store graph for visualization tab
                        st.session_state.agent_graph = agent._graph
                        
                        # Run the agent ‚Äî use existing event loop to avoid
                        # unclosed-loop ResourceWarnings from asyncio.run()
                        loop = asyncio.get_event_loop()
                        result = loop.run_until_complete(agent.run(
                            prompt,
                            st.session_state.chat_history[:-1],
                            reasoning_mode=reasoning_mode
                        ))
                        
                        status_placeholder.empty()
                        
                        # Update final answer
                        if isinstance(result, ReasoningResponse):
                            if result.success:
                                assistant_message = f"**Result:**\n```\n{result.final_output}\n```"
                            else:
                                assistant_message = f"‚ùå {result.error}\n\nLast output:\n```\n{result.final_output or 'No output'}\n```"
                        else:
                            st.session_state.last_context = result.retrieved_chunks
                            st.session_state.matched_concepts = result.matched_concepts
                            assistant_message = result.answer
                        
                        message_placeholder.markdown(assistant_message)
                        
                        # Show model attribution
                        model_used = getattr(result, "model_name", "unknown")
                        st.caption(f"Generated by: `{model_used}`")
                        
                        st.session_state.chat_history.append({
                            "role": "assistant", 
                            "content": assistant_message,
                            "model": model_used
                        })
                        
                        # Trigger rerun to sync everything
                        st.rerun()
                        
                    except Exception as e:
                        status_placeholder.error("‚ùå Error")
                        message_placeholder.error(f"I encountered an error: {str(e)}")
                        import traceback
                        print(f"Agent error: {traceback.format_exc()}")

def render_context_panel():
    """Renders the right-hand context panel."""
    st.markdown("### üìö Retrieved Context")
    
    # REQ-01: Show matched graph concepts
    if st.session_state.get('matched_concepts'):
        with st.expander("üï∏Ô∏è Graph-Matched Concepts", expanded=True):
            concepts = st.session_state.matched_concepts[:8]  # Limit display
            st.markdown(" ‚Ä¢ ".join([f"`{c}`" for c in concepts]))
    
    if st.session_state.last_context:
        for i, chunk in enumerate(st.session_state.last_context):
            # Determine source type
            source_type = chunk.get('source', 'vector')
            source_icon = "üîç" if source_type == 'vector' else "üï∏Ô∏è"
            
            with st.expander(f"{source_icon} Source {i+1}", expanded=(i == 0)):
                # Show scores and source
                original_sim = chunk.get('similarity', 0)
                rerank_score = chunk.get('rerank_score')
                
                # Source badge
                source_badge = "Vector Search" if source_type == 'vector' else "Graph Retrieval"
                st.caption(f"**{source_badge}**")
                
                if rerank_score is not None:
                    st.caption(f"Original: `{original_sim:.3f}` ‚Üí Re-ranked: `{rerank_score:.3f}`")
                else:
                    st.caption(f"Similarity: `{original_sim:.3f}`")
                
                st.markdown(chunk.get('content', '')[:500] + ("..." if len(chunk.get('content', '')) > 500 else ""))
                
                # REQ-VIS-04 / REQ-VIS-05: View Source Buttons
                file_path = chunk.get('file_path')
                metadata = chunk.get('metadata', {})
                
                if file_path:
                    # Construct local static link
                    if "static/" in file_path:
                        relative_path = file_path.split("static/")[1]
                    else:
                        # Fallback: Check if file exists in static/uploads/
                        possible_upload_path = Path("static/uploads") / file_path
                        if possible_upload_path.exists():
                            relative_path = f"uploads/{file_path}"
                        else:
                            relative_path = os.path.basename(file_path) 
                        
                    file_url = f"app/static/{relative_path}"
                    
                    # Layout buttons
                    col1, col2 = st.columns([1, 1])
                    
                    # 1. View PDF
                    if file_path.lower().endswith('.pdf'):
                        page = metadata.get('page_number', 1)
                        with col1:
                            st.link_button("üìÑ View PDF", f"{file_url}#page={page}")
                            
                    # 2. View Code / Text
                    elif file_path.lower().endswith(('.py', '.md', '.txt')):
                        start_line = metadata.get('start_line')
                        end_line = metadata.get('end_line')
                        
                        with col2:
                            if st.button("üìù View Code", key=f"view_{i}_{chunk.get('id')}"):
                                show_code_dialog(file_path, start_line, end_line)

def show_code_dialog(file_path, start_line, end_line):
    with st.dialog("Source Code", width="large"):
        try:
            with open(file_path, 'r') as f:
                lines = f.readlines()
                
            # Highlight specific lines
            if start_line and end_line:
                # Provide some context around lines
                ctx_start = max(0, start_line - 5)
                ctx_end = min(len(lines), end_line + 5)
                
                code_segment = "".join(lines[ctx_start:ctx_end])
                st.caption(f"Showing lines {ctx_start+1} - {ctx_end}")
                st.code(code_segment, language='python' if file_path.endswith('.py') else 'markdown')
            else:
                st.code("".join(lines), language='python' if file_path.endswith('.py') else 'markdown')
                
        except Exception as e:
            st.error(f"Could not read file: {e}")
